paste("for tau =  0.9, Optimal λ=", round(best_lambda, 4)),
paste("λ = ", lambda_1),
"True function = median"
),
col = c("red", "blue", "green"),
lty = c(1, 1, 2),
lwd = c(2, 2, 2))
# -> we want to find lambda that represents the data (not too small lambda that will overfit the data(model) nor too large that oversmooth the model )
# --- PARAMETERS
k_folds <- 5         # split the data into 5 equal folds (1 for validation, 2-5 for training)
taus <- 0.9          # quantile of interest
psi <- 0.05          # smoothing param used during fit (small)
sigma <- 1           # fixed
#Define lambda
lambda_grid <- 10^seq(-2, 5, length.out = 20)  # grid of lambdas to try (10^-2 to 10^5)
set.seed(42)
# --- K-Fold indices ---
n <- nrow(dat)
fold_indexs <- sample(rep(1:k_folds, length.out = n))
# store CV results
cv_results <- data.frame(lambda = lambda_grid, mean_val_loss = NA_real_)
# Precompute starting coefficients from full-data OLS (warm start)
start_coef <- coef(fit)  # from lm(y ~ X - 1)
# --- loop over lambda values in grid (K-Fold) ---
for (i in seq_along(lambda_grid)){
lambda <- lambda_grid[i]
validation_losses <- numeric(k_folds)
for (fold in 1:k_folds) {
train_index <- which(fold_indexs != fold)
validation_index   <- which(fold_indexs == fold)
X_train <- X[train_index,          , drop = FALSE]
y_train <- dat$y[train_index]
X_val   <- X[validation_index,     , drop = FALSE]
y_val   <- dat$y[validation_index]
#OLS on training fold
ols_train <- lm(y_train ~ X_train -1)
start_beta <- coef(ols_train)
if (length(start_beta) != ncol(X)) start_beta <- rep(0, ncol(X))
# FIT on training fold (with penalty lambda)
fit_fold <- optim(
par = start_beta,
fn = pen_ssqr,
X = X_train,
y = y_train,
S = S,
lambda = lambda,
tau = tau,
method = "BFGS",
control = list(maxit = 1000, reltol = 1e-6)
)
beta_hat <- fit_fold$par
# Validation: compute mean pinball loss per observation (NO penalty)
resid_val <- as.numeric (y_val - X_val %*% beta_hat)    # y - mu
pinball_vec <- (tau - (resid_val < 0)) * resid_val
validation_losses[fold] <- mean(pinball_vec)   # mean per-observation
}
# store mean validation loss (lower is better) (normalized per-observation)
cv_results$mean_val_loss[i] <- mean(validation_losses)/ length(validation_index)
}
#choosing best lambda (minimal mean validation)
best_row <- cv_results[which.min(cv_results$mean_val_loss),  ]
best_lambda <- best_row$lambda
print(paste("Best Lambda is: ", best_lambda))
paste("CV loss at optimal lambda is", round(best_row, 6))
# Refit final model on FULL DATA with best lambda
# Warm start: OLS on full data
ols_full <- lm(dat$y ~ X - 1)
start_full <- coef(ols_full)
if (length(start_full) != ncol(X)) start_full <- rep(0, ncol(X))
# Fit final model
fit_final_with_best_lambda <- optim(par = start_full,
fn = pen_ssqr,
y = dat$y,
X = X,
tau = tau,
S = S,
lambda = best_lambda,
method = "BFGS")
plot(x, y, col = "grey")
lines(dat$x, as.numeric(X %*% fit_final_with_best_lambda$par), col = "red", lwd = 2)
#lines(dat$x, as.numeric(X %*% fit_pen_1$par), col = "blue", lwd = 2)
lines(x, sin(x), col = "green", lty = 2, lwd = 2) # True function
legend("topright",
legend = c(
paste("for tau =", tau," | Optimal λ=", round(best_lambda, 4)),
paste("λ = ", lambda_1),
"True function = median"
),
col = c("red", "blue", "green"),
lty = c(1, 1, 2),
lwd = c(2, 2, 2))
# -> we want to find lambda that represents the data (not too small lambda that will overfit the data(model) nor too large that oversmooth the model )
# --- PARAMETERS
k_folds <- 5         # split the data into 5 equal folds (1 for validation, 2-5 for training)
taus <- 0.9          # quantile of interest
psi <- 0.05          # smoothing param used during fit (small)
sigma <- 1           # fixed
#Define lambda
lambda_grid <- 10^seq(-2, 5, length.out = 20)  # grid of lambdas to try (10^-2 to 10^5)
set.seed(42)
# --- K-Fold indices ---
n <- nrow(dat)
fold_indexs <- sample(rep(1:k_folds, length.out = n))
# store CV results
cv_results <- data.frame(lambda = lambda_grid, mean_val_loss = NA_real_)
# Precompute starting coefficients from full-data OLS (warm start)
start_coef <- coef(fit)  # from lm(y ~ X - 1)
# --- loop over lambda values in grid (K-Fold) ---
for (i in seq_along(lambda_grid)){
lambda <- lambda_grid[i]
validation_losses <- numeric(k_folds)
for (fold in 1:k_folds) {
train_index <- which(fold_indexs != fold)
validation_index   <- which(fold_indexs == fold)
X_train <- X[train_index,          , drop = FALSE]
y_train <- dat$y[train_index]
X_val   <- X[validation_index,     , drop = FALSE]
y_val   <- dat$y[validation_index]
#OLS on training fold
ols_train <- lm(y_train ~ X_train -1)
start_beta <- coef(ols_train)
if (length(start_beta) != ncol(X)) start_beta <- rep(0, ncol(X))
# FIT on training fold (with penalty lambda)
fit_fold <- optim(
par = start_beta,
fn = pen_ssqr,
X = X_train,
y = y_train,
S = S,
lambda = lambda,
tau = tau,
method = "BFGS",
control = list(maxit = 1000, reltol = 1e-6)
)
beta_hat <- fit_fold$par
# Validation: compute mean pinball loss per observation (NO penalty)
resid_val <- as.numeric (y_val - X_val %*% beta_hat)    # y - mu
pinball_vec <- (tau - (resid_val < 0)) * resid_val
validation_losses[fold] <- mean(pinball_vec)   # mean per-observation
}
# store mean validation loss (lower is better) (normalized per-observation)
cv_results$mean_val_loss[i] <- mean(validation_losses)/ length(validation_index)
}
#choosing best lambda (minimal mean validation)
best_row <- cv_results[which.min(cv_results$mean_val_loss),  ]
best_lambda <- best_row$lambda
print(paste("Best Lambda is: ", best_lambda), "\n")
# -> we want to find lambda that represents the data (not too small lambda that will overfit the data(model) nor too large that oversmooth the model )
# --- PARAMETERS
k_folds <- 5         # split the data into 5 equal folds (1 for validation, 2-5 for training)
taus <- 0.9          # quantile of interest
psi <- 0.05          # smoothing param used during fit (small)
sigma <- 1           # fixed
#Define lambda
lambda_grid <- 10^seq(-2, 5, length.out = 20)  # grid of lambdas to try (10^-2 to 10^5)
set.seed(42)
# --- K-Fold indices ---
n <- nrow(dat)
fold_indexs <- sample(rep(1:k_folds, length.out = n))
# store CV results
cv_results <- data.frame(lambda = lambda_grid, mean_val_loss = NA_real_)
# Precompute starting coefficients from full-data OLS (warm start)
start_coef <- coef(fit)  # from lm(y ~ X - 1)
# --- loop over lambda values in grid (K-Fold) ---
for (i in seq_along(lambda_grid)){
lambda <- lambda_grid[i]
validation_losses <- numeric(k_folds)
for (fold in 1:k_folds) {
train_index <- which(fold_indexs != fold)
validation_index   <- which(fold_indexs == fold)
X_train <- X[train_index,          , drop = FALSE]
y_train <- dat$y[train_index]
X_val   <- X[validation_index,     , drop = FALSE]
y_val   <- dat$y[validation_index]
#OLS on training fold
ols_train <- lm(y_train ~ X_train -1)
start_beta <- coef(ols_train)
if (length(start_beta) != ncol(X)) start_beta <- rep(0, ncol(X))
# FIT on training fold (with penalty lambda)
fit_fold <- optim(
par = start_beta,
fn = pen_ssqr,
X = X_train,
y = y_train,
S = S,
lambda = lambda,
tau = tau,
method = "BFGS",
control = list(maxit = 1000, reltol = 1e-6)
)
beta_hat <- fit_fold$par
# Validation: compute mean pinball loss per observation (NO penalty)
resid_val <- as.numeric (y_val - X_val %*% beta_hat)    # y - mu
pinball_vec <- (tau - (resid_val < 0)) * resid_val
validation_losses[fold] <- mean(pinball_vec)   # mean per-observation
}
# store mean validation loss (lower is better) (normalized per-observation)
cv_results$mean_val_loss[i] <- mean(validation_losses)/ length(validation_index)
}
#choosing best lambda (minimal mean validation)
best_row <- cv_results[which.min(cv_results$mean_val_loss),  ]
best_lambda <- best_row$lambda
print(paste("Best Lambda is: ", best_lambda))
paste("CV loss at optimal lambda is", round(best_row, 6))
#comparing CV loss between optimal lambda and lambda =100
lambda_100_row <- cv_results[cv_results$lambda == 100,]
lambda_100_loss <- lambda_100_row$mean_val_loss
paste("CV Loss at λ = 100 =", round(lambda_100_loss, 6))
# Refit final model on FULL DATA with best lambda
# Warm start: OLS on full data
ols_full <- lm(dat$y ~ X - 1)
start_full <- coef(ols_full)
if (length(start_full) != ncol(X)) start_full <- rep(0, ncol(X))
# Fit final model
fit_final_with_best_lambda <- optim(par = start_full,
fn = pen_ssqr,
y = dat$y,
X = X,
tau = tau,
S = S,
lambda = best_lambda,
method = "BFGS")
plot(x, y, col = "grey")
lines(dat$x, as.numeric(X %*% fit_final_with_best_lambda$par), col = "red", lwd = 2)
#lines(dat$x, as.numeric(X %*% fit_pen_1$par), col = "blue", lwd = 2)
lines(x, sin(x), col = "green", lty = 2, lwd = 2) # True function
legend("topright",
legend = c(
paste("for tau =", tau," | Optimal λ=", round(best_lambda, 4)),
paste("λ = ", lambda_1),
"True function = median"
),
col = c("red", "blue", "green"),
lty = c(1, 1, 2),
lwd = c(2, 2, 2))
lambda_100_loss
cv_results$lambda
# -> we want to find lambda that represents the data (not too small lambda that will overfit the data(model) nor too large that oversmooth the model )
# --- PARAMETERS
k_folds <- 5         # split the data into 5 equal folds (1 for validation, 2-5 for training)
taus <- 0.9          # quantile of interest
psi <- 0.05          # smoothing param used during fit (small)
sigma <- 1           # fixed
#Define lambda
lambda_grid <- 10^seq(-2, 5, length.out = 20)  # grid of lambdas to try (10^-2 to 10^5)
set.seed(42)
# --- K-Fold indices ---
n <- nrow(dat)
fold_indexs <- sample(rep(1:k_folds, length.out = n))
# store CV results
cv_results <- data.frame(lambda = lambda_grid, mean_val_loss = NA_real_)
# Precompute starting coefficients from full-data OLS (warm start)
start_coef <- coef(fit)  # from lm(y ~ X - 1)
# --- loop over lambda values in grid (K-Fold) ---
for (i in seq_along(lambda_grid)){
lambda <- lambda_grid[i]
validation_losses <- numeric(k_folds)
for (fold in 1:k_folds) {
train_index <- which(fold_indexs != fold)
validation_index   <- which(fold_indexs == fold)
X_train <- X[train_index,          , drop = FALSE]
y_train <- dat$y[train_index]
X_val   <- X[validation_index,     , drop = FALSE]
y_val   <- dat$y[validation_index]
#OLS on training fold
ols_train <- lm(y_train ~ X_train -1)
start_beta <- coef(ols_train)
if (length(start_beta) != ncol(X)) start_beta <- rep(0, ncol(X))
# FIT on training fold (with penalty lambda)
fit_fold <- optim(
par = start_beta,
fn = pen_ssqr,
X = X_train,
y = y_train,
S = S,
lambda = lambda,
tau = tau,
method = "BFGS",
control = list(maxit = 1000, reltol = 1e-6)
)
beta_hat <- fit_fold$par
# Validation: compute mean pinball loss per observation (NO penalty)
resid_val <- as.numeric (y_val - X_val %*% beta_hat)    # y - mu
pinball_vec <- (tau - (resid_val < 0)) * resid_val
validation_losses[fold] <- mean(pinball_vec)   # mean per-observation
}
# store mean validation loss (lower is better) (normalized per-observation)
cv_results$mean_val_loss[i] <- mean(validation_losses)/ length(validation_index)
}
#choosing best lambda (minimal mean validation)
best_row <- cv_results[which.min(cv_results$mean_val_loss),  ]
best_lambda <- best_row$lambda
print(paste("Best Lambda is: ", best_lambda))
paste("CV loss at optimal lambda is", round(best_row, 6))
#comparing CV loss between optimal lambda and other lambda in
lambda_100_row <- cv_results[cv_results$lambda,]
lambda_100_loss <- lambda_100_row$mean_val_loss
paste("CV Loss at λ = 100 =", round(lambda_100_loss, 6))
# Refit final model on FULL DATA with best lambda
# Warm start: OLS on full data
ols_full <- lm(dat$y ~ X - 1)
start_full <- coef(ols_full)
if (length(start_full) != ncol(X)) start_full <- rep(0, ncol(X))
# Fit final model
fit_final_with_best_lambda <- optim(par = start_full,
fn = pen_ssqr,
y = dat$y,
X = X,
tau = tau,
S = S,
lambda = best_lambda,
method = "BFGS")
plot(x, y, col = "grey")
lines(dat$x, as.numeric(X %*% fit_final_with_best_lambda$par), col = "red", lwd = 2)
#lines(dat$x, as.numeric(X %*% fit_pen_1$par), col = "blue", lwd = 2)
lines(x, sin(x), col = "green", lty = 2, lwd = 2) # True function
legend("topright",
legend = c(
paste("for tau =", tau," | Optimal λ=", round(best_lambda, 4)),
paste("λ = ", lambda_1),
"True function = median"
),
col = c("red", "blue", "green"),
lty = c(1, 1, 2),
lwd = c(2, 2, 2))
get_loss <- function(lam) {
idx <- which(cv_results$lambda == lam)
if (length(idx) == 0) return(NA)
return(cv_results$mean_val_loss[idx])
}
get_loss(1)
get_loss <- function(lam) {
idx <- which(cv_results$lambda == lam)
if (length(idx) == 0) return(NA)
return(cv_results$mean_val_loss[idx])
}
get_loss(100)
cv_results$lambda
best_row
View(get_loss)
View(cv_results)
# -> we want to find lambda that represents the data (not too small lambda that will overfit the data(model) nor too large that oversmooth the model )
# --- PARAMETERS
k_folds <- 5         # split the data into 5 equal folds (1 for validation, 2-5 for training)
taus <- 0.9          # quantile of interest
psi <- 0.05          # smoothing param used during fit (small)
sigma <- 1           # fixed
#Define lambda
lambda_grid <- 10^seq(-3, 10, length.out = 20)  # grid of lambdas to try (10^-2 to 10^5)
set.seed(42)
# --- K-Fold indices ---
n <- nrow(dat)
fold_indexs <- sample(rep(1:k_folds, length.out = n))
# store CV results
cv_results <- data.frame(lambda = lambda_grid, mean_val_loss = NA_real_)
# Precompute starting coefficients from full-data OLS (warm start)
start_coef <- coef(fit)  # from lm(y ~ X - 1)
# --- loop over lambda values in grid (K-Fold) ---
for (i in seq_along(lambda_grid)){
lambda <- lambda_grid[i]
validation_losses <- numeric(k_folds)
for (fold in 1:k_folds) {
train_index <- which(fold_indexs != fold)
validation_index   <- which(fold_indexs == fold)
X_train <- X[train_index,          , drop = FALSE]
y_train <- dat$y[train_index]
X_val   <- X[validation_index,     , drop = FALSE]
y_val   <- dat$y[validation_index]
#OLS on training fold
ols_train <- lm(y_train ~ X_train -1)
start_beta <- coef(ols_train)
if (length(start_beta) != ncol(X)) start_beta <- rep(0, ncol(X))
# FIT on training fold (with penalty lambda)
fit_fold <- optim(
par = start_beta,
fn = pen_ssqr,
X = X_train,
y = y_train,
S = S,
lambda = lambda,
tau = tau,
method = "BFGS",
control = list(maxit = 1000, reltol = 1e-6)
)
beta_hat <- fit_fold$par
# Validation: compute mean pinball loss per observation (NO penalty)
resid_val <- as.numeric (y_val - X_val %*% beta_hat)    # y - mu
pinball_vec <- (tau - (resid_val < 0)) * resid_val
validation_losses[fold] <- mean(pinball_vec)   # mean per-observation
}
# store mean validation loss (lower is better) (normalized per-observation)
cv_results$mean_val_loss[i] <- mean(validation_losses)/ length(validation_index)
}
#choosing best lambda (minimal mean validation)
best_row <- cv_results[which.min(cv_results$mean_val_loss),  ]
best_lambda <- best_row$lambda
print(paste("Best Lambda is: ", best_lambda))
paste("CV loss at optimal lambda is", round(best_row, 6))
#The optimal lambda is when the CV loss is the minimum
# Refit final model on FULL DATA with best lambda
# Warm start: OLS on full data
ols_full <- lm(dat$y ~ X - 1)
start_full <- coef(ols_full)
if (length(start_full) != ncol(X)) start_full <- rep(0, ncol(X))
# Fit final model
fit_final_with_best_lambda <- optim(par = start_full,
fn = pen_ssqr,
y = dat$y,
X = X,
tau = tau,
S = S,
lambda = best_lambda,
method = "BFGS")
plot(x, y, col = "grey")
lines(dat$x, as.numeric(X %*% fit_final_with_best_lambda$par), col = "red", lwd = 2)
#lines(dat$x, as.numeric(X %*% fit_pen_1$par), col = "blue", lwd = 2)
lines(x, sin(x), col = "green", lty = 2, lwd = 2) # True function
legend("topright",
legend = c(
paste("for tau =", tau," | Optimal λ=", round(best_lambda, 4)),
paste("λ = ", lambda_1),
"True function = median"
),
col = c("red", "blue", "green"),
lty = c(1, 1, 2),
lwd = c(2, 2, 2))
# -> we want to find lambda that represents the data (not too small lambda that will overfit the data(model) nor too large that oversmooth the model )
# --- PARAMETERS
k_folds <- 5         # split the data into 5 equal folds (1 for validation, 2-5 for training)
taus <- 0.9          # quantile of interest
psi <- 0.05          # smoothing param used during fit (small)
sigma <- 1           # fixed
#Define lambda
lambda_grid <- 10^seq(-3, 10, length.out = 20)  # grid of lambdas to try (10^-2 to 10^5)
set.seed(42)
# --- K-Fold indices ---
n <- nrow(dat)
fold_indexs <- sample(rep(1:k_folds, length.out = n))
# store CV results
cv_results <- data.frame(lambda = lambda_grid, mean_val_loss = NA_real_)
# Precompute starting coefficients from full-data OLS (warm start)
start_coef <- coef(fit)  # from lm(y ~ X - 1)
# --- loop over lambda values in grid (K-Fold) ---
for (i in seq_along(lambda_grid)){
lambda <- lambda_grid[i]
validation_losses <- numeric(k_folds)
for (fold in 1:k_folds) {
train_index <- which(fold_indexs != fold)
validation_index   <- which(fold_indexs == fold)
X_train <- X[train_index,          , drop = FALSE]
y_train <- dat$y[train_index]
X_val   <- X[validation_index,     , drop = FALSE]
y_val   <- dat$y[validation_index]
#OLS on training fold
ols_train <- lm(y_train ~ X_train -1)
start_beta <- coef(ols_train)
if (length(start_beta) != ncol(X)) start_beta <- rep(0, ncol(X))
# FIT on training fold (with penalty lambda)
fit_fold <- optim(
par = start_beta,
fn = pen_ssqr,
X = X_train,
y = y_train,
S = S,
lambda = lambda,
tau = tau,
method = "BFGS",
control = list(maxit = 1000, reltol = 1e-6)
)
beta_hat <- fit_fold$par
# Validation: compute mean pinball loss per observation (NO penalty)
resid_val <- as.numeric (y_val - X_val %*% beta_hat)    # y - mu
pinball_vec <- (tau - (resid_val < 0)) * resid_val
validation_losses[fold] <- mean(pinball_vec)   # mean per-observation
}
# store mean validation loss (lower is better) (normalized per-observation)
cv_results$mean_val_loss[i] <- mean(validation_losses)/ length(validation_index)
}
#choosing best lambda (minimal mean validation)
best_row <- cv_results[which.min(cv_results$mean_val_loss),  ]
best_lambda <- best_row$lambda
print(paste("Best Lambda is: ", best_lambda))
paste("CV loss at optimal lambda is", round(best_row, 6))
#The optimal lambda is when the CV loss is the minimum
# Refit final model on FULL DATA with best lambda
# Warm start: OLS on full data
ols_full <- lm(dat$y ~ X - 1)
start_full <- coef(ols_full)
if (length(start_full) != ncol(X)) start_full <- rep(0, ncol(X))
# Fit final model
fit_final_with_best_lambda <- optim(par = start_full,
fn = pen_ssqr,
y = dat$y,
X = X,
tau = tau,
S = S,
lambda = best_lambda,
method = "BFGS")
plot(x, y, col = "grey")
lines(dat$x, as.numeric(X %*% fit_final_with_best_lambda$par), col = "red", lwd = 2)
lines(dat$x, as.numeric(X %*% fit_pen_1$par), col = "blue", lwd = 2)
lines(x, sin(x), col = "green", lty = 2, lwd = 2) # True function
legend("topright",
legend = c(
paste("for tau =", tau," | Optimal λ=", round(best_lambda, 4)),
paste("λ = ", lambda_1),
"True function = median"
),
col = c("red", "blue", "green"),
lty = c(1, 1, 2),
lwd = c(2, 2, 2))
View(cv_results)
