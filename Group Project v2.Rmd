---
title: "Group Project"
author: |
  By Nicholas Xu, Kevin Sutikno and Xuanlin Zhu
output:
  pdf_document:
    number_sections: yes
    fig_height: 4
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require("knitr")
```

```{r echo=FALSE}
set.seed(12345)
```


```{r}
n <- 1e3
x <- seq(0, 2*pi, length.out = n)
y <- sin(x) + rnorm(n, 0, 0.5)
dat <- data.frame(x = x, y = y)
plot(x, y)
```

```{r}
library(mgcv)
sm <- smoothCon(s(x, k = 10, bs = "cr"), data=dat, knots=NULL)[[1]]
X <- sm$X
S <- sm$S[[1]] # penalty matrix S

fit <- lm(y ~ X-1, data = dat)

matplot(X, x = x, type = 'l', ylab = "Spline basis function")
```

```{r}
# Computes ELF density and its derivatives w.r.t mu
dlf <- function(x, tau, mu, psi, log = FALSE, deriv = 0)
{
  sig <- 1
  y <- (x - mu) / sig
  out <- (1-tau) * y - psi * log(1 + exp(y / psi)) - log(sig * psi * beta(psi * (1 - tau), tau * psi))
  if (!log) out <- exp(out)
  if(deriv > 0)
  {
    out <- list("d" = out)
    dl <- dlogis(x, mu, psi*sig)
    pl <- plogis(x, mu, psi*sig)
    out$D <- sum((pl - (1-tau)) / sig)
    if(deriv > 1)
    {
      out$D2 <- sum(-dl / sig)
    }
  }
  return(out)
}

# Computes negative log-likelihood as a function of beta
negllkFun <- function(par, tau, psi, dat, X)
{
  mu <- X %*% par
  out <- - sum( dlf(x = dat, tau = tau, mu = mu, psi = psi, log = TRUE))
  return(out)
}

# Computes derivative of the log-likelihood w.r.t. beta
negGrad <- function(par, tau, psi, dat, X)
{
  mu <- X %*% par
  tmp <- lapply(1:length(mu),
                function(ii){
                  a <- - dlf(x = dat[ii], tau = tau, mu = mu[ii],
                  psi = psi, log = TRUE, deriv = 1)$D
                  return(a[1] * X[ii, ])
                }
  )
  out <- Reduce("+", tmp)
  return(out)
}
```

```{r}
# version one of penalty
pen_negllk <- function(par, tau, psi, dat, X, S, lambda) {
  negllkFun(par, tau, psi, dat, X) + as.numeric(lambda * t(par) %*% (S %*% par))
}

pen_negGrad <- function(par, tau, psi, dat, X, S, lambda) {
  as.numeric(negGrad(par, tau, psi, dat, X)) + 2 * lambda * as.vector(S %*% par)
}

# second version of penalty
pen_ssqr <- function(beta, X, y, S, 
                     lambda,tau, psi =0.1, sigma =1){
  mu <- X %*% beta
  res <- (y-mu)
  pen <- as.numeric(t(beta) %*% S %*% beta) #produces a 1x1 matrix
  
  loss <- sum((tau -(res<0))* res) 
  penalty <- lambda * pen
  return(loss+penalty)
}
```

```{r}
tau <- 0.9 #90th quantile
psi <- 0.1
lambda_1 <- 100
sigma <- 1


fit_pen_1 <- optim(par = coef(fit),
                fn = pen_negllk,
                gr = pen_negGrad,
                tau = tau, psi = psi, dat = dat$y, X = X, S = S, lambda = lambda_1,
                method = "BFGS")

fit_pen_2 <- optim(par = coef(fit),
                 fn = pen_ssqr,
                 y = y, X = X,
                 tau = tau, S = S,
                 lambda = lambda_1, method = "BFGS",
                 )

plot(x, y, col = "grey", main = paste("Quantile Regression tau =", tau))
lines(x, X %*% fit_pen_1$par, col = "red", lwd = 2)
lines(x, X %*% fit_pen_2$par, col = "purple", lwd = 2)
lines(x, sin(x), col = "blue", lty = 2, lwd = 2) # True function
legend("topright", legend = c("Data", "Fitted", "True"), 
       col = c("grey", "red", "blue"), lty = c(NA, 1, 2), pch = c(1, NA, NA))
```

##Doing K-Fold Cross Validation to find optimal lambda that captures the model smoothly
```{r}
# -> we want to find lambda that represents the data (not too small lambda that will overfit the data(model) nor too large that oversmooth the model )

# --- PARAMETERS
k_folds <- 5         # split the data into 5 equal folds (1 for validation, 2-5 for training)
taus <- 0.9          # quantile of interest
psi <- 0.05          # smoothing param used during fit (small)
sigma <- 1           # fixed
#Define lambda
lambda_grid <- 10^seq(-3, 10, length.out = 20)  # grid of lambdas to try (10^-2 to 10^5)
set.seed(42)


# --- K-Fold indices ---
n <- nrow(dat)
fold_indexs <- sample(rep(1:k_folds, length.out = n))

# store CV results
cv_results <- data.frame(lambda = lambda_grid, mean_val_loss = NA_real_)

# Precompute starting coefficients from full-data OLS (warm start)
start_coef <- coef(fit)  # from lm(y ~ X - 1)

# --- loop over lambda values in grid (K-Fold) ---
for (i in seq_along(lambda_grid)){
  lambda <- lambda_grid[i]
  validation_losses <- numeric(k_folds)
  
  for (fold in 1:k_folds) {
      train_index <- which(fold_indexs != fold)
      validation_index   <- which(fold_indexs == fold)
      
      X_train <- X[train_index,          , drop = FALSE]
      y_train <- dat$y[train_index]

      X_val   <- X[validation_index,     , drop = FALSE]
      y_val   <- dat$y[validation_index]
      
    
      
      #OLS on training fold
      ols_train <- lm(y_train ~ X_train -1)
      start_beta <- coef(ols_train)
      if (length(start_beta) != ncol(X)) start_beta <- rep(0, ncol(X))
      
      
      # FIT on training fold (with penalty lambda)
      fit_fold <- optim(
                      par = start_beta,
                      fn = pen_ssqr,
                      X = X_train,
                      y = y_train,
                      S = S,
                      lambda = lambda,
                      tau = tau,
                      method = "BFGS",
                      control = list(maxit = 1000, reltol = 1e-6)
                    )
      
      beta_hat <- fit_fold$par
      
    # Validation: compute mean pinball loss per observation (NO penalty)
    resid_val <- as.numeric (y_val - X_val %*% beta_hat)    # y - mu
    pinball_vec <- (tau - (resid_val < 0)) * resid_val
    validation_losses[fold] <- mean(pinball_vec)   # mean per-observation
  
  }
      
      # store mean validation loss (lower is better) (normalized per-observation)
      cv_results$mean_val_loss[i] <- mean(validation_losses)/ length(validation_index) 
}
  
  #choosing best lambda (minimal mean validation)
  best_row <- cv_results[which.min(cv_results$mean_val_loss),  ]
  best_lambda <- best_row$lambda
  print(paste("Best Lambda is: ", best_lambda))
  paste("CV loss at optimal lambda is", round(best_row, 6))
  
  #The optimal lambda is when the CV loss is the minimum
  
  
  
# Refit final model on FULL DATA with best lambda
# Warm start: OLS on full data
ols_full <- lm(dat$y ~ X - 1)
start_full <- coef(ols_full)
if (length(start_full) != ncol(X)) start_full <- rep(0, ncol(X))
  
 # Fit final model
  fit_final_with_best_lambda <- optim(par = start_full,
                            fn = pen_ssqr,
                            y = dat$y,
                            X = X,
                            tau = tau,
                            S = S,
                            lambda = best_lambda,
                            method = "BFGS")
  
  plot(x, y, col = "grey")
  
  lines(dat$x, as.numeric(X %*% fit_final_with_best_lambda$par), col = "red", lwd = 2)
  lines(dat$x, as.numeric(X %*% fit_pen_1$par), col = "blue", lwd = 2)
  lines(x, sin(x), col = "green", lty = 2, lwd = 2) # True function
  
  legend("topright",
       legend = c(
         paste("for tau =", tau," | Optimal λ=", round(best_lambda, 4)),
         paste("λ = ", lambda_1),
         "True function = median"
       ),
       col = c("red", "blue", "green"),
       lty = c(1, 1, 2),
       lwd = c(2, 2, 2))
  

```

```{r}
# testing our code

```
